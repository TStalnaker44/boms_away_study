{"0": {"meta": {"StartDate": "2023-03-01 16:33:34", "EndDate": "2023-03-01 17:01:03", "Status": "IP Address", "Progress": "100", "Duration": "1649", "Finished": "True", "RecordedDate": "2023-03-01 17:01:04", "ResponseID": "R_snX1LGF2NUe2M6Z", "UserLanguage": "EN"}, "background": {"B1": "No", "B2": {"answers": [""], "other": ""}, "B3": {"answer": "", "list": ""}}, "aibom_fields": {"AI1": "", "AI2": {"answers": ["Environment in which the model was trained in", "Model structure / architecture", "Model hyperparameters", "Known model/data defects", "Potential model biases", "Potential fairness issues", "Optimizers, loss functions, etc.", "Description of the training data", "Description of the validation and testing data", "Runtime performance requirements", "Data (pre-)processing techniques", "Presence of Personally Identifiable Information", "Model version", "Model description", "Creation / building time", "Model license", "Data version", "System dependencies", "Others (please specify)"], "other": "Specific model definitions - say I'm working in PyTorch and want to use someone else's pretrained weights. I should have access to the exact model definition used for the model. If someone is forced to reverse engineer the model, small details may be incorrect, leading to errors. For example, classification heads for image classifiers are usually either in the order of AvgPool2d -> norm -> fc or norm->AvgPool2d -> fc. Reversing the order has no impact on the loading of the model weights, but leads to differences in model performance.\nValidation procedures are also potentially in need of documentation, specifically the image preprocessing pipeline used during the validation phase of image classification models should be fully documented such that it is replicable from scratch. I know that cropping procedures, resizing interpolation algorithms, and other small and often-overlooked aspects of deep learning code can have effects on the results."}, "AI3": "", "AI4": "", "AI5": {"answer": "No", "list": ""}, "AI6": {"1": "3", "2": "1", "3": "2", "4": "4", "5": "5", "6": "6", "other": ""}, "AI7": "I think that a push and shift toward increased documentation and the use of AIBOMs, or at least some of their components, will help report dependencies. This can be seen in the past 5 or so years, where there is a large increase of paper authors publishing code and including a list of hyperparameters, training recipes, software package versions, hardware used, and other details about experiments in papers. I think that as top venues start to require more of these documentations, there will be a shift to where publishing AIBOMs becomes the norm."}, "databom_fields": {"D1": "Agree", "D2": {"answer": "", "list": ""}, "D3": {"answers": ["Data sources", "Data pre-processing steps", "Data transformations", "Dataset authors / creators", "Data labelers / taggers", "Procedure for data collection and curation", "Presence of Personally Identifiable Information", "Dataset statistics", "Dataset version", "Dataset license", "Dataset description", "Version description (dataset change log)", "Creation time", "Known data biases", "Known vulnerabilities and issues", "Unique identifier", "Types of data included in the dataset (image, text, binary, etc)", "Dataset size", "Others (please specify)"], "other": "Known problems in a dataset ie. corrupt images, tips for using the dataset, including prewritten codebases for working with the data ie. repository of pytorch dataset definitions and sample training scripts for use with a newly proposed image dataset, visualization/data exporation tools, hosted access ie. huggingface datasets"}, "D4": {"answer": "No", "list": ""}, "D5": "If you are proposing a dataset, make a dataBOM along with it. if you are using data, link to the dataset using a resource that also has the dataBOM accessible. If you are doing any sort of training in the process of working with data, create an AIBOM describing this training. I think that they should be separate overall, with each serving to describe a different category of things.", "D6": {"1": "3", "2": "1", "3": "2", "4": "4", "5": "5", "other": ""}, "D7": "Sample code will make data easier to work with, as will hosting. I should not have to go a scrape a dataset using a csv of flickr URLs.", "D8": "Some of the fields that would ideally be present are difficult to complete, requiring domain specific knowledge. If I'm good with image models and happen to also want to propose a new dataset, it is difficult for me alone to do all the analysis to create a full DataBOM. I will need people experienced in different domains, such as ethics, data science, and data visualization.", "D9": "again, by making a communitywide push for these thing. I believe that the neurips dataset track requires a dataset card to be submitted alongside the paper, which includes some basic parts of a dataBOM. Expanding this requirement and the contends of this dataset card should help increase the use of DataBOMs."}, "challenges": {"C1": "increased reproducibility, easier iteration on previous works, more accessible resources, earlier detection of weaknesses/vulnerabilities.", "C2": "Time consuming, requires developers to write documentation, may require domain specific knowledge and specific or niche skillsets", "C3": {"answer": "I don't know", "list": ""}, "C4": "", "C5": {"answer": "", "list": ""}, "C6": "", "C7": "I think they should echo the latest legal stances from different jurisdictions, as well as any personal opinions/objections the authors may have to particular uses of a dataset."}, "demographics": {"Q1": "2", "Q2": {"answer": "Other (please specify)", "other": "researcher"}, "Q3": {"answer": "Other (please specify)", "other": "Some college but as part of my high school education"}, "Q4": {"answers": ["Python", "Java", "C / C++"], "other": ""}, "Q5": {"answer": "Deep Learning models (CNNs, RNNs, Transformers, etc.)", "other": ""}, "Q6": "Open source only", "Q7": "content retrieval", "Q8": "No", "Q9": "Yes, informal", "Q10": "Never"}}, "1": {"meta": {"StartDate": "2023-03-01 17:01:43", "EndDate": "2023-03-01 17:20:36", "Status": "IP Address", "Progress": "100", "Duration": "1132", "Finished": "True", "RecordedDate": "2023-03-01 17:20:36", "ResponseID": "R_2nSZ6Uk4gpCWUxP", "UserLanguage": "EN"}, "background": {"B1": "No", "B2": {"answers": [""], "other": ""}, "B3": {"answer": "", "list": ""}}, "aibom_fields": {"AI1": "", "AI2": {"answers": ["Optimizers, loss functions, etc.", "Description of the training data", "Description of the validation and testing data", "Data (pre-)processing techniques", "Others (please specify)"], "other": "The loss function is the most important. The loss function is also directly related to the data it was trained on. "}, "AI3": "", "AI4": "", "AI5": {"answer": "No", "list": ""}, "AI6": {"1": "6", "2": "1", "3": "4", "4": "5", "5": "3", "6": "2", "other": "Written into the model parameters"}, "AI7": "I can only think of being able to reproduce the model to be completely sure of how it was created."}, "databom_fields": {"D1": "Neutral", "D2": {"answer": "", "list": ""}, "D3": {"answers": ["Data sources", "Data pre-processing steps", "Data transformations", "Dataset authors / creators", "Data labelers / taggers", "Procedure for data collection and curation", "Presence of Personally Identifiable Information", "Dataset version", "Dataset license", "Dataset description", "Version description (dataset change log)", "Creation time"], "other": ""}, "D4": {"answer": "No", "list": ""}, "D5": "The data is what defines what the model will learn. So knowing about the data half the story of a model.", "D6": {"1": "4", "2": "1", "3": "2", "4": "3", "5": "5", "other": ""}, "D7": "I think people already try to document their datasets. having a structure will help them to not forget to detail specific aspects of the dataset. Like Nature Scientific Data provides a structure that is very useful.", "D8": "Ridged requirements are always annoying when they don't apply to your work. Enforcing a standard will likely hurt innovation more than help it.", "D9": "Create a legal liability for misrepresenting data."}, "challenges": {"C1": "It will provide structure to help people document their work.", "C2": "It is hard to anticipate how an imposed structure will hinder someones work.", "C3": {"answer": "No", "list": ""}, "C4": "", "C5": {"answer": "No", "list": ""}, "C6": "", "C7": "If the AIBOM specifies that there is no issue with licensed material and there turns out to be an issue, then the legal liability will rest with the creator of the AIBOM who misrepresented the claims."}, "demographics": {"Q1": "8", "Q2": {"answer": "Other (please specify)", "other": "Researcher"}, "Q3": {"answer": "Doctoral Degree", "other": ""}, "Q4": {"answers": ["Python", "Java", "C / C++", "Javascript"], "other": ""}, "Q5": {"answer": "Deep Learning models (CNNs, RNNs, Transformers, etc.)", "other": ""}, "Q6": "Open and closed source", "Q7": "Healthcare, security, conservation, assistive technology", "Q8": "Yes, formal", "Q9": "Yes, informal", "Q10": "Quarterly"}}, "2": {"meta": {"StartDate": "2023-03-01 17:13:09", "EndDate": "2023-03-01 17:20:56", "Status": "IP Address", "Progress": "100", "Duration": "467", "Finished": "True", "RecordedDate": "2023-03-01 17:20:57", "ResponseID": "R_3DbEw6tOraj96Gs", "UserLanguage": "EN"}, "background": {"B1": "No", "B2": {"answers": [""], "other": ""}, "B3": {"answer": "", "list": ""}}, "aibom_fields": {"AI1": "", "AI2": {"answers": ["Environment in which the model was trained in", "Model structure / architecture", "Model hyperparameters", "Model parameters", "Known model/data defects", "Unique model identifier", "Optimizers, loss functions, etc.", "Description of the training data", "Description of the validation and testing data", "Runtime performance requirements", "Data (pre-)processing techniques", "Presence of Personally Identifiable Information", "Model version", "Model description", "Creation / building time", "Model license", "Data version", "System dependencies"], "other": ""}, "AI3": "", "AI4": "", "AI5": {"answer": "Yes (please list them)", "list": "HuggingFace does this to a certain degree when you train models with their Trainer and push them to the hub with push_to_hub. The README is populated with a lot of information that was mentioned in the previous question. "}, "AI6": {"1": "4", "2": "1", "3": "2", "4": "5", "5": "3", "6": "6", "other": ""}, "AI7": "Should contain virtually everything mentioned in the last question. Even then reproducing results could be tricky."}, "databom_fields": {"D1": "Agree", "D2": {"answer": "", "list": ""}, "D3": {"answers": ["Data sources", "Data pre-processing steps", "Data transformations", "Procedure for data collection and curation", "Presence of Personally Identifiable Information", "Dataset statistics", "Dataset version", "Dataset license", "Dataset description", "Version description (dataset change log)", "Creation time", "Known data biases", "Known vulnerabilities and issues", "Unique identifier", "Types of data included in the dataset (image, text, binary, etc)", "Dataset size"], "other": ""}, "D4": {"answer": "Yes (please list them)", "list": "Again, HuggingFace does this to a certain degree with the dataset cards on the HF hub"}, "D5": "You would need both to reproduce results from an ML system, so they should be considered complementary.", "D6": {"1": "3", "2": "1", "3": "2", "4": "4", "5": "5", "other": ""}, "D7": "Often data preprocessing steps that are critical for good performance / reproduction of results go unmentioned in papers. A DataBOM could help make these more transparent.", "D8": "Getting dataset creators to fill them in. Should be automated as much as possible.", "D9": "Perhaps requiring them to publish papers in conferences and journals."}, "challenges": {"C1": "Similar answer to DataBOMs.", "C2": "Similar answer to DataBOMs.", "C3": {"answer": "I don't know", "list": ""}, "C4": "", "C5": {"answer": "", "list": ""}, "C6": "", "C7": "Maybe provide a license information for each data source. Also provide information allowing data owners to opt out from scrapping."}, "demographics": {"Q1": "5", "Q2": {"answer": "Data scientist", "other": ""}, "Q3": {"answer": "Doctoral Degree", "other": ""}, "Q4": {"answers": ["Python"], "other": ""}, "Q5": {"answer": "Deep Learning models (CNNs, RNNs, Transformers, etc.)", "other": ""}, "Q6": "Open source only", "Q7": "Mainly healthcare and biology", "Q8": "No", "Q9": "No", "Q10": "Annually"}}}